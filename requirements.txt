# Core dependencies for Nemotron Next 8B LoRA Fine-tuning with FiQA

# NeMo AutoModel
nemo-automodel>=0.2.0

# Transformers and PEFT
transformers>=4.40.0
peft>=0.8.0
accelerate>=0.30.0

# Data processing
datasets>=2.16.0

# Training utilities
torch>=2.0.0
tensorboard>=2.15.0

# Optional but recommended
numpy>=1.24.0
tqdm>=4.66.0
