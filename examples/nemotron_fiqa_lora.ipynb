{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Nemotron Next 8B LoRA Fine-tuning with FiQA Dataset\n",
        "\n",
        "This notebook demonstrates LoRA fine-tuning of [NVIDIA Nemotron Next 8B](https://huggingface.co/nvidia/Nemotron-Next-8B) on the [FiQA dataset](https://huggingface.co/datasets/explodinggradients/fiqa) for financial question answering.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Setup & Environment](#setup)\n",
        "2. [Model Loading](#model-loading)\n",
        "3. [Dataset Loading & Preprocessing](#dataset)\n",
        "4. [Baseline Evaluation](#baseline)\n",
        "5. [LoRA Configuration](#lora-config)\n",
        "6. [LoRA Training](#training)\n",
        "7. [Fine-tuned Evaluation](#evaluation)\n",
        "8. [Visualization & Analysis](#visualization)\n",
        "\n",
        "---\n",
        "\n",
        "## GPU Requirements\n",
        "\n",
        "‚ö†Ô∏è **This notebook requires a GPU with 24GB+ VRAM** (A100, H100, or RTX 4090 recommended)\n",
        "\n",
        "| Phase | GPU Required | Time Estimate |\n",
        "|-------|--------------|---------------|\n",
        "| Model Loading | ‚úÖ Yes | 2-5 min |\n",
        "| Dataset Prep | ‚úÖ Yes | 10-25 min |\n",
        "| Baseline Eval | ‚úÖ Yes | 30-60 min |\n",
        "| LoRA Training | ‚úÖ Yes | 3-6 hours |\n",
        "| Final Eval | ‚úÖ Yes | 30-60 min |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name=\"setup\"></a>\n",
        "## 1. Setup & Environment\n",
        "\n",
        "First, let's verify our environment and import required libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU REQUIRED - Verify CUDA availability\n",
        "import torch\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"Environment Check\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    raise RuntimeError(\"‚ùå CUDA not available! This notebook requires a GPU.\")\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"‚úÖ GPU environment verified!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "# Data processing\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# NeMo AutoModel imports\n",
        "try:\n",
        "    import nemo_automodel\n",
        "    from nemo_automodel._transformers import NeMoAutoModelForCausalLM\n",
        "    from nemo_automodel.components._peft.lora import PeftConfig, apply_lora_to_linear_modules\n",
        "    print(f\"‚úÖ NeMo AutoModel imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ö†Ô∏è NeMo AutoModel not found: {e}\")\n",
        "    print(\"Please install: cd Automodel && uv pip install -e .\")\n",
        "    raise\n",
        "\n",
        "# Transformers for tokenizer\n",
        "from transformers import AutoTokenizer, AutoProcessor\n",
        "\n",
        "# Evaluation\n",
        "import evaluate\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "CONFIG = {\n",
        "    # Model\n",
        "    \"model_name\": \"nvidia/Nemotron-Next-8B\",\n",
        "    \"torch_dtype\": torch.bfloat16,\n",
        "    \n",
        "    # Dataset\n",
        "    \"dataset_name\": \"explodinggradients/fiqa\",\n",
        "    \"train_split_ratio\": 0.8,  # 80% train, 20% validation from original train\n",
        "    \"max_length\": 512,\n",
        "    \n",
        "    # LoRA\n",
        "    \"lora_rank\": 8,\n",
        "    \"lora_alpha\": 32,\n",
        "    \"lora_dropout\": 0.05,\n",
        "    \n",
        "    # Training\n",
        "    \"learning_rate\": 2e-4,\n",
        "    \"batch_size\": 4,\n",
        "    \"num_epochs\": 3,\n",
        "    \"gradient_accumulation_steps\": 4,\n",
        "    \"warmup_ratio\": 0.1,\n",
        "    \n",
        "    # Paths\n",
        "    \"output_dir\": \"./outputs\",\n",
        "    \"checkpoint_dir\": \"./checkpoints\",\n",
        "}\n",
        "\n",
        "# Create output directories\n",
        "Path(CONFIG[\"output_dir\"]).mkdir(parents=True, exist_ok=True)\n",
        "Path(CONFIG[\"checkpoint_dir\"]).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Configuration loaded\")\n",
        "print(f\"   Model: {CONFIG['model_name']}\")\n",
        "print(f\"   Dataset: {CONFIG['dataset_name']}\")\n",
        "print(f\"   LoRA rank: {CONFIG['lora_rank']}, alpha: {CONFIG['lora_alpha']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"model-loading\"></a>\n",
        "## 2. Model Loading\n",
        "\n",
        "**‚è±Ô∏è Time Estimate: 2-5 minutes** | **GPU REQUIRED**\n",
        "\n",
        "Load Nemotron Next 8B using NeMo AutoModel APIs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU REQUIRED - Load Nemotron Next 8B\n",
        "# ‚è±Ô∏è Time: 2-5 minutes\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"Loading Nemotron Next 8B...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Load tokenizer\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    CONFIG[\"model_name\"],\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Ensure padding token is set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(f\"‚úÖ Tokenizer loaded\")\n",
        "print(f\"   Vocab size: {tokenizer.vocab_size}\")\n",
        "print(f\"   Pad token: {tokenizer.pad_token}\")\n",
        "\n",
        "# Load model using NeMo AutoModel\n",
        "print(\"\\nLoading model (this may take a few minutes)...\")\n",
        "model = NeMoAutoModelForCausalLM.from_pretrained(\n",
        "    CONFIG[\"model_name\"],\n",
        "    torch_dtype=CONFIG[\"torch_dtype\"],\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "load_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n‚úÖ Model loaded successfully in {load_time:.1f} seconds\")\n",
        "print(f\"   Model type: {type(model).__name__}\")\n",
        "print(f\"   Dtype: {model.dtype}\")\n",
        "print(f\"   Device: {next(model.parameters()).device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print model architecture summary\n",
        "def count_parameters(model):\n",
        "    \"\"\"Count trainable and total parameters.\"\"\"\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return total_params, trainable_params\n",
        "\n",
        "total, trainable = count_parameters(model)\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"Model Summary\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Total parameters: {total:,} ({total/1e9:.2f}B)\")\n",
        "print(f\"Trainable parameters: {trainable:,} ({trainable/1e9:.2f}B)\")\n",
        "print(f\"Trainable %: {100 * trainable / total:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test basic inference to verify model works\n",
        "print(\"=\" * 50)\n",
        "print(\"Testing Model Inference\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Simple financial question to test\n",
        "test_prompt = \"\"\"You are a helpful financial assistant.\n",
        "\n",
        "Question: What is the difference between a stock and a bond?\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "# Tokenize\n",
        "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate\n",
        "print(\"Generating response...\")\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "\n",
        "# Decode\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Test Prompt:\")\n",
        "print(\"=\" * 50)\n",
        "print(test_prompt)\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Model Response:\")\n",
        "print(\"=\" * 50)\n",
        "print(response[len(test_prompt):].strip())\n",
        "print(\"\\n‚úÖ Model inference test passed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"dataset\"></a>\n",
        "## 3. Dataset Loading & Preprocessing\n",
        "\n",
        "**‚è±Ô∏è Time Estimate: 10-25 minutes** | **GPU REQUIRED**\n",
        "\n",
        "Load FiQA dataset and format for instruction fine-tuning.\n",
        "\n",
        "### FiQA Dataset Info\n",
        "- **Source**: [explodinggradients/fiqa](https://huggingface.co/datasets/explodinggradients/fiqa)\n",
        "- **Splits**: Train (5,500), Validation (500), Test (648)\n",
        "- **Format**: Question + Ground Truth Answers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load FiQA dataset\n",
        "# ‚è±Ô∏è Time: ~1 minute\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"Loading FiQA Dataset\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Load dataset with 'main' config\n",
        "fiqa_dataset = load_dataset(\"explodinggradients/fiqa\", \"main\")\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded!\")\n",
        "print(f\"\\nSplits:\")\n",
        "for split_name, split_data in fiqa_dataset.items():\n",
        "    print(f\"  {split_name}: {len(split_data):,} samples\")\n",
        "\n",
        "print(f\"\\nColumns: {fiqa_dataset['train'].column_names}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore sample data\n",
        "print(\"=\" * 50)\n",
        "print(\"Sample Data\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "sample = fiqa_dataset['train'][0]\n",
        "print(f\"\\nQuestion:\\n{sample['question']}\")\n",
        "print(f\"\\nGround Truth Answer (first of {len(sample['ground_truths'])}):\")\n",
        "print(sample['ground_truths'][0][:500] + \"...\" if len(sample['ground_truths'][0]) > 500 else sample['ground_truths'][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format data for instruction fine-tuning\n",
        "def format_for_training(example: Dict) -> Dict:\n",
        "    \"\"\"\n",
        "    Format FiQA sample for instruction fine-tuning.\n",
        "    \n",
        "    Input format:\n",
        "        - question: str\n",
        "        - ground_truths: List[str]\n",
        "    \n",
        "    Output format:\n",
        "        - text: formatted instruction string\n",
        "        - input_text: question only (for inference)\n",
        "        - target_text: answer only (for evaluation)\n",
        "    \"\"\"\n",
        "    question = example['question']\n",
        "    # Use first ground truth answer (most relevant)\n",
        "    answer = example['ground_truths'][0] if example['ground_truths'] else \"\"\n",
        "    \n",
        "    # Truncate very long answers to avoid context length issues\n",
        "    max_answer_len = 1024\n",
        "    if len(answer) > max_answer_len:\n",
        "        answer = answer[:max_answer_len] + \"...\"\n",
        "    \n",
        "    # Format as instruction-following\n",
        "    instruction = f\"\"\"You are a helpful financial assistant. Answer the following question about finance.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer: {answer}\"\"\"\n",
        "    \n",
        "    # Also store components separately for evaluation\n",
        "    input_prompt = f\"\"\"You are a helpful financial assistant. Answer the following question about finance.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    \n",
        "    return {\n",
        "        \"text\": instruction,\n",
        "        \"input_text\": input_prompt,\n",
        "        \"target_text\": answer,\n",
        "        \"question\": question,\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Formatting function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply formatting to all splits\n",
        "# ‚è±Ô∏è Time: ~1 minute\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"Formatting Dataset\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "formatted_dataset = fiqa_dataset.map(\n",
        "    format_for_training,\n",
        "    remove_columns=['ground_truths'],  # Keep only new columns\n",
        "    desc=\"Formatting samples\"\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Dataset formatted!\")\n",
        "print(f\"New columns: {formatted_dataset['train'].column_names}\")\n",
        "\n",
        "# Show formatted example\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Formatted Sample\")\n",
        "print(\"=\" * 50)\n",
        "print(formatted_dataset['train'][0]['text'][:800] + \"...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize dataset\n",
        "# ‚è±Ô∏è Time: 5-15 minutes\n",
        "\n",
        "def tokenize_function(examples: Dict) -> Dict:\n",
        "    \"\"\"Tokenize examples for training.\"\"\"\n",
        "    # Tokenize the full text (question + answer)\n",
        "    tokenized = tokenizer(\n",
        "        examples['text'],\n",
        "        truncation=True,\n",
        "        max_length=CONFIG['max_length'],\n",
        "        padding='max_length',\n",
        "        return_tensors=None,\n",
        "    )\n",
        "    \n",
        "    # For causal LM, labels are same as input_ids\n",
        "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
        "    \n",
        "    return tokenized\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"Tokenizing Dataset\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "tokenized_dataset = formatted_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['text', 'input_text', 'target_text', 'question'],\n",
        "    desc=\"Tokenizing\"\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Dataset tokenized!\")\n",
        "print(f\"Columns: {tokenized_dataset['train'].column_names}\")\n",
        "print(f\"Sample input_ids length: {len(tokenized_dataset['train'][0]['input_ids'])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset summary\n",
        "print(\"=\" * 50)\n",
        "print(\"Dataset Preparation Complete\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"\\nüìä Dataset Statistics:\")\n",
        "print(f\"   Train samples: {len(tokenized_dataset['train']):,}\")\n",
        "print(f\"   Validation samples: {len(tokenized_dataset['validation']):,}\")\n",
        "print(f\"   Test samples: {len(tokenized_dataset['test']):,}\")\n",
        "print(f\"   Max sequence length: {CONFIG['max_length']}\")\n",
        "\n",
        "# Keep the formatted dataset for evaluation later\n",
        "eval_dataset = formatted_dataset  # Has input_text and target_text\n",
        "\n",
        "print(\"\\n‚úÖ Phase 3: Dataset Loading & Preprocessing complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"baseline\"></a>\n",
        "## 4. Baseline Evaluation\n",
        "\n",
        "**‚è±Ô∏è Time Estimate: 30-60 minutes** | **GPU REQUIRED**\n",
        "\n",
        "Evaluate the base model on FiQA test set before fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Baseline Evaluation Setup\n",
        "# ‚è±Ô∏è Time: ~30-60 minutes depending on test set size\n",
        "\n",
        "# Use a subset for baseline to save time\n",
        "BASELINE_SAMPLE_SIZE = 100  # Evaluate on 100 samples for speed\n",
        "\n",
        "# Select random samples from test set\n",
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "test_indices = random.sample(range(len(eval_dataset['test'])), min(BASELINE_SAMPLE_SIZE, len(eval_dataset['test'])))\n",
        "baseline_test_samples = eval_dataset['test'].select(test_indices)\n",
        "\n",
        "print(f\"üìä Baseline evaluation on {len(baseline_test_samples)} test samples\")\n",
        "print(f\"   (Full test set has {len(eval_dataset['test'])} samples)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define inference function\n",
        "def generate_response(prompt: str, max_new_tokens: int = 256) -> str:\n",
        "    \"\"\"Generate response from model given a prompt.\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=CONFIG['max_length'])\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    \n",
        "    # Decode only the generated part (exclude input)\n",
        "    input_length = inputs['input_ids'].shape[1]\n",
        "    generated_tokens = outputs[0][input_length:]\n",
        "    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "    \n",
        "    return response.strip()\n",
        "\n",
        "print(\"‚úÖ Inference function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define evaluation metrics\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    \"\"\"Normalize text for comparison.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "def compute_exact_match(prediction: str, reference: str) -> float:\n",
        "    \"\"\"Compute exact match score (0 or 1).\"\"\"\n",
        "    return float(normalize_text(prediction) == normalize_text(reference))\n",
        "\n",
        "def compute_f1(prediction: str, reference: str) -> float:\n",
        "    \"\"\"Compute token-level F1 score.\"\"\"\n",
        "    pred_tokens = normalize_text(prediction).split()\n",
        "    ref_tokens = normalize_text(reference).split()\n",
        "    \n",
        "    if len(pred_tokens) == 0 or len(ref_tokens) == 0:\n",
        "        return float(pred_tokens == ref_tokens)\n",
        "    \n",
        "    common = Counter(pred_tokens) & Counter(ref_tokens)\n",
        "    num_common = sum(common.values())\n",
        "    \n",
        "    if num_common == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    precision = num_common / len(pred_tokens)\n",
        "    recall = num_common / len(ref_tokens)\n",
        "    f1 = 2 * precision * recall / (precision + recall)\n",
        "    \n",
        "    return f1\n",
        "\n",
        "def compute_bleu_simple(prediction: str, reference: str, max_n: int = 4) -> float:\n",
        "    \"\"\"Compute simple BLEU score (without brevity penalty for simplicity).\"\"\"\n",
        "    pred_tokens = normalize_text(prediction).split()\n",
        "    ref_tokens = normalize_text(reference).split()\n",
        "    \n",
        "    if len(pred_tokens) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    scores = []\n",
        "    for n in range(1, min(max_n + 1, len(pred_tokens) + 1)):\n",
        "        pred_ngrams = Counter([tuple(pred_tokens[i:i+n]) for i in range(len(pred_tokens) - n + 1)])\n",
        "        ref_ngrams = Counter([tuple(ref_tokens[i:i+n]) for i in range(len(ref_tokens) - n + 1)])\n",
        "        \n",
        "        matches = sum((pred_ngrams & ref_ngrams).values())\n",
        "        total = sum(pred_ngrams.values())\n",
        "        \n",
        "        if total > 0:\n",
        "            scores.append(matches / total)\n",
        "        else:\n",
        "            scores.append(0.0)\n",
        "    \n",
        "    if not scores or all(s == 0 for s in scores):\n",
        "        return 0.0\n",
        "    \n",
        "    # Geometric mean\n",
        "    from functools import reduce\n",
        "    import operator\n",
        "    non_zero = [s for s in scores if s > 0]\n",
        "    if not non_zero:\n",
        "        return 0.0\n",
        "    return reduce(operator.mul, non_zero, 1) ** (1 / len(non_zero))\n",
        "\n",
        "print(\"‚úÖ Evaluation metrics defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run baseline evaluation\n",
        "# ‚è±Ô∏è Time: 30-60 minutes (depends on GPU and sample size)\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"Running Baseline Evaluation (Base Model)\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Samples: {len(baseline_test_samples)}\")\n",
        "print(\"This may take a while...\")\n",
        "print()\n",
        "\n",
        "baseline_results = []\n",
        "\n",
        "for idx, sample in enumerate(tqdm(baseline_test_samples, desc=\"Evaluating\")):\n",
        "    input_prompt = sample['input_text']\n",
        "    reference = sample['target_text']\n",
        "    \n",
        "    # Generate response\n",
        "    prediction = generate_response(input_prompt)\n",
        "    \n",
        "    # Compute metrics\n",
        "    em = compute_exact_match(prediction, reference)\n",
        "    f1 = compute_f1(prediction, reference)\n",
        "    bleu = compute_bleu_simple(prediction, reference)\n",
        "    \n",
        "    baseline_results.append({\n",
        "        'question': sample['question'],\n",
        "        'reference': reference,\n",
        "        'prediction': prediction,\n",
        "        'exact_match': em,\n",
        "        'f1': f1,\n",
        "        'bleu': bleu,\n",
        "    })\n",
        "    \n",
        "    # Show progress every 10 samples\n",
        "    if (idx + 1) % 10 == 0:\n",
        "        avg_f1 = sum(r['f1'] for r in baseline_results) / len(baseline_results)\n",
        "        print(f\"  Progress: {idx + 1}/{len(baseline_test_samples)} | Running F1: {avg_f1:.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Baseline evaluation complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute and display baseline metrics\n",
        "baseline_df = pd.DataFrame(baseline_results)\n",
        "\n",
        "baseline_metrics = {\n",
        "    'exact_match': baseline_df['exact_match'].mean(),\n",
        "    'f1': baseline_df['f1'].mean(),\n",
        "    'bleu': baseline_df['bleu'].mean(),\n",
        "}\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"üìä BASELINE METRICS (Pre-Fine-tuning)\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"\\n  Exact Match:  {baseline_metrics['exact_match']:.4f}\")\n",
        "print(f\"  F1 Score:     {baseline_metrics['f1']:.4f}\")\n",
        "print(f\"  BLEU Score:   {baseline_metrics['bleu']:.4f}\")\n",
        "print()\n",
        "\n",
        "# Show a few example predictions\n",
        "print(\"=\" * 50)\n",
        "print(\"Sample Predictions\")\n",
        "print(\"=\" * 50)\n",
        "for i in range(min(3, len(baseline_results))):\n",
        "    result = baseline_results[i]\n",
        "    print(f\"\\nüìå Sample {i+1}:\")\n",
        "    print(f\"   Question: {result['question'][:100]}...\")\n",
        "    print(f\"   Reference: {result['reference'][:150]}...\")\n",
        "    print(f\"   Prediction: {result['prediction'][:150]}...\")\n",
        "    print(f\"   F1: {result['f1']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save baseline results for later comparison\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Create outputs directory\n",
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "\n",
        "# Save baseline metrics\n",
        "baseline_save_path = \"outputs/baseline_metrics.json\"\n",
        "with open(baseline_save_path, 'w') as f:\n",
        "    json.dump({\n",
        "        'metrics': baseline_metrics,\n",
        "        'sample_size': len(baseline_results),\n",
        "        'model_name': CONFIG['model_name'],\n",
        "    }, f, indent=2)\n",
        "\n",
        "# Save detailed results\n",
        "baseline_results_path = \"outputs/baseline_results.csv\"\n",
        "baseline_df.to_csv(baseline_results_path, index=False)\n",
        "\n",
        "print(f\"‚úÖ Baseline metrics saved to: {baseline_save_path}\")\n",
        "print(f\"‚úÖ Detailed results saved to: {baseline_results_path}\")\n",
        "print(f\"\\n‚úÖ Phase 4: Baseline Evaluation complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"lora-config\"></a>\n",
        "## 5. LoRA Configuration\n",
        "\n",
        "**‚è±Ô∏è Time Estimate: ~5 minutes** | **GPU REQUIRED**\n",
        "\n",
        "Configure and apply LoRA adapter to the model using PEFT (Parameter-Efficient Fine-Tuning).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import PEFT for LoRA\n",
        "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"LoRA Configuration\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Define LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=CONFIG['lora_r'],                    # Rank of the update matrices\n",
        "    lora_alpha=CONFIG['lora_alpha'],       # Scaling factor\n",
        "    lora_dropout=CONFIG['lora_dropout'],   # Dropout probability\n",
        "    target_modules=[                        # Which modules to apply LoRA to\n",
        "        \"q_proj\",\n",
        "        \"k_proj\", \n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    bias=\"none\",                           # Don't train bias terms\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ LoRA Config:\")\n",
        "print(f\"   Rank (r): {lora_config.r}\")\n",
        "print(f\"   Alpha: {lora_config.lora_alpha}\")\n",
        "print(f\"   Dropout: {lora_config.lora_dropout}\")\n",
        "print(f\"   Target modules: {lora_config.target_modules}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply LoRA adapter to the model\n",
        "print(\"=\" * 50)\n",
        "print(\"Applying LoRA Adapter\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Enable gradient checkpointing for memory efficiency\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "print(\"‚úÖ LoRA adapter applied to model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify trainable parameters\n",
        "print(\"=\" * 50)\n",
        "print(\"Trainable Parameters\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Print trainable parameters summary\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Detailed breakdown\n",
        "trainable_params = 0\n",
        "all_params = 0\n",
        "for name, param in model.named_parameters():\n",
        "    all_params += param.numel()\n",
        "    if param.requires_grad:\n",
        "        trainable_params += param.numel()\n",
        "\n",
        "print(f\"\\nüìä Parameter Summary:\")\n",
        "print(f\"   Total parameters:     {all_params:,}\")\n",
        "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"   Trainable %:          {100 * trainable_params / all_params:.4f}%\")\n",
        "print(f\"   Memory reduction:     ~{100 - 100 * trainable_params / all_params:.1f}%\")\n",
        "\n",
        "print(f\"\\n‚úÖ Phase 5: LoRA Configuration complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"training\"></a>\n",
        "## 6. LoRA Training\n",
        "\n",
        "**‚è±Ô∏è Time Estimate: 3-6 hours** | **GPU REQUIRED**\n",
        "\n",
        "Train the LoRA adapter on FiQA training data.\n",
        "\n",
        "> üöß **TODO: Phase 6** - Implement LoRA training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"evaluation\"></a>\n",
        "## 7. Fine-tuned Evaluation\n",
        "\n",
        "**‚è±Ô∏è Time Estimate: 30-60 minutes** | **GPU REQUIRED**\n",
        "\n",
        "Evaluate the fine-tuned model and compare with baseline.\n",
        "\n",
        "> üöß **TODO: Phase 7** - Implement fine-tuned evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"visualization\"></a>\n",
        "## 8. Visualization & Analysis\n",
        "\n",
        "Create visualizations comparing baseline vs fine-tuned performance.\n",
        "\n",
        "> üöß **TODO: Phase 8** - Implement visualization and analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook demonstrated LoRA fine-tuning of Nemotron Next 8B on the FiQA financial QA dataset.\n",
        "\n",
        "### Results\n",
        "\n",
        "| Metric | Baseline | Fine-tuned | Improvement |\n",
        "|--------|----------|------------|-------------|\n",
        "| Exact Match | TBD | TBD | TBD |\n",
        "| F1 Score | TBD | TBD | TBD |\n",
        "| BLEU | TBD | TBD | TBD |\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Experiment with different LoRA ranks\n",
        "- Try longer training\n",
        "- Evaluate on additional financial QA datasets\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
