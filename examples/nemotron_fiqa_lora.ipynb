{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Nemotron Next 8B LoRA Fine-tuning with FiQA Dataset\n",
        "\n",
        "This notebook demonstrates LoRA fine-tuning of [NVIDIA Nemotron Next 8B](https://huggingface.co/nvidia/Nemotron-Next-8B) on the [FiQA dataset](https://huggingface.co/datasets/explodinggradients/fiqa) for financial question answering.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Setup & Environment](#setup)\n",
        "2. [Model Loading](#model-loading)\n",
        "3. [Dataset Loading & Preprocessing](#dataset)\n",
        "4. [Baseline Evaluation](#baseline)\n",
        "5. [LoRA Configuration](#lora-config)\n",
        "6. [LoRA Training](#training)\n",
        "7. [Fine-tuned Evaluation](#evaluation)\n",
        "8. [Visualization & Analysis](#visualization)\n",
        "\n",
        "---\n",
        "\n",
        "## GPU Requirements\n",
        "\n",
        "‚ö†Ô∏è **This notebook requires a GPU with 24GB+ VRAM** (A100, H100, or RTX 4090 recommended)\n",
        "\n",
        "| Phase | GPU Required | Time Estimate |\n",
        "|-------|--------------|---------------|\n",
        "| Model Loading | ‚úÖ Yes | 2-5 min |\n",
        "| Dataset Prep | ‚úÖ Yes | 10-25 min |\n",
        "| Baseline Eval | ‚úÖ Yes | 30-60 min |\n",
        "| LoRA Training | ‚úÖ Yes | 3-6 hours |\n",
        "| Final Eval | ‚úÖ Yes | 30-60 min |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name=\"setup\"></a>\n",
        "## 1. Setup & Environment\n",
        "\n",
        "First, let's verify our environment and import required libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU REQUIRED - Verify CUDA availability\n",
        "import torch\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"Environment Check\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    raise RuntimeError(\"‚ùå CUDA not available! This notebook requires a GPU.\")\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"‚úÖ GPU environment verified!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "# Data processing\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# NeMo AutoModel imports\n",
        "try:\n",
        "    import nemo_automodel\n",
        "    from nemo_automodel._transformers import NeMoAutoModelForCausalLM\n",
        "    from nemo_automodel.components._peft.lora import PeftConfig, apply_lora_to_linear_modules\n",
        "    print(f\"‚úÖ NeMo AutoModel imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ö†Ô∏è NeMo AutoModel not found: {e}\")\n",
        "    print(\"Please install: cd Automodel && uv pip install -e .\")\n",
        "    raise\n",
        "\n",
        "# Transformers for tokenizer\n",
        "from transformers import AutoTokenizer, AutoProcessor\n",
        "\n",
        "# Evaluation\n",
        "import evaluate\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "CONFIG = {\n",
        "    # Model\n",
        "    \"model_name\": \"nvidia/Nemotron-Next-8B\",\n",
        "    \"torch_dtype\": torch.bfloat16,\n",
        "    \n",
        "    # Dataset\n",
        "    \"dataset_name\": \"explodinggradients/fiqa\",\n",
        "    \"train_split_ratio\": 0.8,  # 80% train, 20% validation from original train\n",
        "    \"max_length\": 512,\n",
        "    \n",
        "    # LoRA\n",
        "    \"lora_rank\": 8,\n",
        "    \"lora_alpha\": 32,\n",
        "    \"lora_dropout\": 0.05,\n",
        "    \n",
        "    # Training\n",
        "    \"learning_rate\": 2e-4,\n",
        "    \"batch_size\": 4,\n",
        "    \"num_epochs\": 3,\n",
        "    \"gradient_accumulation_steps\": 4,\n",
        "    \"warmup_ratio\": 0.1,\n",
        "    \n",
        "    # Paths\n",
        "    \"output_dir\": \"./outputs\",\n",
        "    \"checkpoint_dir\": \"./checkpoints\",\n",
        "}\n",
        "\n",
        "# Create output directories\n",
        "Path(CONFIG[\"output_dir\"]).mkdir(parents=True, exist_ok=True)\n",
        "Path(CONFIG[\"checkpoint_dir\"]).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Configuration loaded\")\n",
        "print(f\"   Model: {CONFIG['model_name']}\")\n",
        "print(f\"   Dataset: {CONFIG['dataset_name']}\")\n",
        "print(f\"   LoRA rank: {CONFIG['lora_rank']}, alpha: {CONFIG['lora_alpha']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"model-loading\"></a>\n",
        "## 2. Model Loading\n",
        "\n",
        "**‚è±Ô∏è Time Estimate: 2-5 minutes** | **GPU REQUIRED**\n",
        "\n",
        "Load Nemotron Next 8B using NeMo AutoModel APIs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU REQUIRED - Load Nemotron Next 8B\n",
        "# ‚è±Ô∏è Time: 2-5 minutes\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"Loading Nemotron Next 8B...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Load tokenizer\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    CONFIG[\"model_name\"],\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Ensure padding token is set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(f\"‚úÖ Tokenizer loaded\")\n",
        "print(f\"   Vocab size: {tokenizer.vocab_size}\")\n",
        "print(f\"   Pad token: {tokenizer.pad_token}\")\n",
        "\n",
        "# Load model using NeMo AutoModel\n",
        "print(\"\\nLoading model (this may take a few minutes)...\")\n",
        "model = NeMoAutoModelForCausalLM.from_pretrained(\n",
        "    CONFIG[\"model_name\"],\n",
        "    torch_dtype=CONFIG[\"torch_dtype\"],\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "load_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n‚úÖ Model loaded successfully in {load_time:.1f} seconds\")\n",
        "print(f\"   Model type: {type(model).__name__}\")\n",
        "print(f\"   Dtype: {model.dtype}\")\n",
        "print(f\"   Device: {next(model.parameters()).device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print model architecture summary\n",
        "def count_parameters(model):\n",
        "    \"\"\"Count trainable and total parameters.\"\"\"\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return total_params, trainable_params\n",
        "\n",
        "total, trainable = count_parameters(model)\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"Model Summary\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Total parameters: {total:,} ({total/1e9:.2f}B)\")\n",
        "print(f\"Trainable parameters: {trainable:,} ({trainable/1e9:.2f}B)\")\n",
        "print(f\"Trainable %: {100 * trainable / total:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test basic inference to verify model works\n",
        "print(\"=\" * 50)\n",
        "print(\"Testing Model Inference\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Simple financial question to test\n",
        "test_prompt = \"\"\"You are a helpful financial assistant.\n",
        "\n",
        "Question: What is the difference between a stock and a bond?\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "# Tokenize\n",
        "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate\n",
        "print(\"Generating response...\")\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "\n",
        "# Decode\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Test Prompt:\")\n",
        "print(\"=\" * 50)\n",
        "print(test_prompt)\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Model Response:\")\n",
        "print(\"=\" * 50)\n",
        "print(response[len(test_prompt):].strip())\n",
        "print(\"\\n‚úÖ Model inference test passed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"dataset\"></a>\n",
        "## 3. Dataset Loading & Preprocessing\n",
        "\n",
        "**‚è±Ô∏è Time Estimate: 10-25 minutes** | **GPU REQUIRED**\n",
        "\n",
        "Load FiQA dataset, create train/val/test splits, and format for instruction fine-tuning.\n",
        "\n",
        "> üöß **TODO: Phase 3** - Implement dataset loading and preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"baseline\"></a>\n",
        "## 4. Baseline Evaluation\n",
        "\n",
        "**‚è±Ô∏è Time Estimate: 30-60 minutes** | **GPU REQUIRED**\n",
        "\n",
        "Evaluate the base model on FiQA test set before fine-tuning.\n",
        "\n",
        "> üöß **TODO: Phase 4** - Implement baseline evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"lora-config\"></a>\n",
        "## 5. LoRA Configuration\n",
        "\n",
        "**‚è±Ô∏è Time Estimate: ~5 minutes** | **GPU REQUIRED**\n",
        "\n",
        "Configure and apply LoRA adapter to the model.\n",
        "\n",
        "> üöß **TODO: Phase 5** - Implement LoRA configuration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"training\"></a>\n",
        "## 6. LoRA Training\n",
        "\n",
        "**‚è±Ô∏è Time Estimate: 3-6 hours** | **GPU REQUIRED**\n",
        "\n",
        "Train the LoRA adapter on FiQA training data.\n",
        "\n",
        "> üöß **TODO: Phase 6** - Implement LoRA training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"evaluation\"></a>\n",
        "## 7. Fine-tuned Evaluation\n",
        "\n",
        "**‚è±Ô∏è Time Estimate: 30-60 minutes** | **GPU REQUIRED**\n",
        "\n",
        "Evaluate the fine-tuned model and compare with baseline.\n",
        "\n",
        "> üöß **TODO: Phase 7** - Implement fine-tuned evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"visualization\"></a>\n",
        "## 8. Visualization & Analysis\n",
        "\n",
        "Create visualizations comparing baseline vs fine-tuned performance.\n",
        "\n",
        "> üöß **TODO: Phase 8** - Implement visualization and analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook demonstrated LoRA fine-tuning of Nemotron Next 8B on the FiQA financial QA dataset.\n",
        "\n",
        "### Results\n",
        "\n",
        "| Metric | Baseline | Fine-tuned | Improvement |\n",
        "|--------|----------|------------|-------------|\n",
        "| Exact Match | TBD | TBD | TBD |\n",
        "| F1 Score | TBD | TBD | TBD |\n",
        "| BLEU | TBD | TBD | TBD |\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Experiment with different LoRA ranks\n",
        "- Try longer training\n",
        "- Evaluate on additional financial QA datasets\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
