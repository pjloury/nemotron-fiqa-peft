{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Nemotron Next 8B LoRA Fine-tuning with FiQA Dataset\n",
        "\n",
        "This notebook demonstrates LoRA fine-tuning of [NVIDIA Nemotron Next 8B](https://huggingface.co/nvidia/Nemotron-Next-8B) on the [FiQA dataset](https://huggingface.co/datasets/explodinggradients/fiqa) for financial question answering.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Setup & Environment](#setup)\n",
        "2. [Model Loading](#model-loading)\n",
        "3. [Dataset Loading & Preprocessing](#dataset)\n",
        "4. [Baseline Evaluation](#baseline)\n",
        "5. [LoRA Configuration](#lora-config)\n",
        "6. [LoRA Training](#training)\n",
        "7. [Fine-tuned Evaluation](#evaluation)\n",
        "8. [Visualization & Analysis](#visualization)\n",
        "\n",
        "---\n",
        "\n",
        "## GPU Requirements\n",
        "\n",
        "‚ö†Ô∏è **This notebook requires a GPU with 24GB+ VRAM** (A100, H100, or RTX 4090 recommended)\n",
        "\n",
        "| Phase | GPU Required | Time Estimate |\n",
        "|-------|--------------|---------------|\n",
        "| Model Loading | ‚úÖ Yes | 2-5 min |\n",
        "| Dataset Prep | ‚úÖ Yes | 10-25 min |\n",
        "| Baseline Eval | ‚úÖ Yes | 30-60 min |\n",
        "| LoRA Training | ‚úÖ Yes | 3-6 hours |\n",
        "| Final Eval | ‚úÖ Yes | 30-60 min |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name=\"setup\"></a>\n",
        "## 1. Setup & Environment\n",
        "\n",
        "First, let's verify our environment and import required libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU REQUIRED - Verify CUDA availability\n",
        "import torch\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"Environment Check\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    raise RuntimeError(\"‚ùå CUDA not available! This notebook requires a GPU.\")\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"‚úÖ GPU environment verified!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "# Data processing\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# NeMo AutoModel imports\n",
        "try:\n",
        "    import nemo_automodel\n",
        "    from nemo_automodel._transformers import NeMoAutoModelForCausalLM\n",
        "    from nemo_automodel.components._peft.lora import PeftConfig, apply_lora_to_linear_modules\n",
        "    print(f\"‚úÖ NeMo AutoModel imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ö†Ô∏è NeMo AutoModel not found: {e}\")\n",
        "    print(\"Please install: cd Automodel && uv pip install -e .\")\n",
        "    raise\n",
        "\n",
        "# Transformers for tokenizer\n",
        "from transformers import AutoTokenizer, AutoProcessor\n",
        "\n",
        "# Evaluation\n",
        "import evaluate\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "CONFIG = {\n",
        "    # Model\n",
        "    \"model_name\": \"nvidia/Nemotron-Next-8B\",\n",
        "    \"torch_dtype\": torch.bfloat16,\n",
        "    \n",
        "    # Dataset\n",
        "    \"dataset_name\": \"explodinggradients/fiqa\",\n",
        "    \"train_split_ratio\": 0.8,  # 80% train, 20% validation from original train\n",
        "    \"max_length\": 512,\n",
        "    \n",
        "    # LoRA\n",
        "    \"lora_rank\": 8,\n",
        "    \"lora_alpha\": 32,\n",
        "    \"lora_dropout\": 0.05,\n",
        "    \n",
        "    # Training\n",
        "    \"learning_rate\": 2e-4,\n",
        "    \"batch_size\": 4,\n",
        "    \"num_epochs\": 3,\n",
        "    \"gradient_accumulation_steps\": 4,\n",
        "    \"warmup_ratio\": 0.1,\n",
        "    \n",
        "    # Paths\n",
        "    \"output_dir\": \"./outputs\",\n",
        "    \"checkpoint_dir\": \"./checkpoints\",\n",
        "}\n",
        "\n",
        "# Create output directories\n",
        "Path(CONFIG[\"output_dir\"]).mkdir(parents=True, exist_ok=True)\n",
        "Path(CONFIG[\"checkpoint_dir\"]).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Configuration loaded\")\n",
        "print(f\"   Model: {CONFIG['model_name']}\")\n",
        "print(f\"   Dataset: {CONFIG['dataset_name']}\")\n",
        "print(f\"   LoRA rank: {CONFIG['lora_rank']}, alpha: {CONFIG['lora_alpha']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"model-loading\"></a>\n",
        "## 2. Model Loading\n",
        "\n",
        "**‚è±Ô∏è Time Estimate: 2-5 minutes** | **GPU REQUIRED**\n",
        "\n",
        "Load Nemotron Next 8B using NeMo AutoModel APIs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU REQUIRED - Load Nemotron Next 8B\n",
        "# ‚è±Ô∏è Time: 2-5 minutes\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"Loading Nemotron Next 8B...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Load tokenizer\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    CONFIG[\"model_name\"],\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Ensure padding token is set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(f\"‚úÖ Tokenizer loaded\")\n",
        "print(f\"   Vocab size: {tokenizer.vocab_size}\")\n",
        "print(f\"   Pad token: {tokenizer.pad_token}\")\n",
        "\n",
        "# Load model using NeMo AutoModel\n",
        "print(\"\\nLoading model (this may take a few minutes)...\")\n",
        "model = NeMoAutoModelForCausalLM.from_pretrained(\n",
        "    CONFIG[\"model_name\"],\n",
        "    torch_dtype=CONFIG[\"torch_dtype\"],\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "load_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n‚úÖ Model loaded successfully in {load_time:.1f} seconds\")\n",
        "print(f\"   Model type: {type(model).__name__}\")\n",
        "print(f\"   Dtype: {model.dtype}\")\n",
        "print(f\"   Device: {next(model.parameters()).device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print model architecture summary\n",
        "def count_parameters(model):\n",
        "    \"\"\"Count trainable and total parameters.\"\"\"\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return total_params, trainable_params\n",
        "\n",
        "total, trainable = count_parameters(model)\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"Model Summary\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Total parameters: {total:,} ({total/1e9:.2f}B)\")\n",
        "print(f\"Trainable parameters: {trainable:,} ({trainable/1e9:.2f}B)\")\n",
        "print(f\"Trainable %: {100 * trainable / total:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test basic inference to verify model works\n",
        "print(\"=\" * 50)\n",
        "print(\"Testing Model Inference\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Simple financial question to test\n",
        "test_prompt = \"\"\"You are a helpful financial assistant.\n",
        "\n",
        "Question: What is the difference between a stock and a bond?\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "# Tokenize\n",
        "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate\n",
        "print(\"Generating response...\")\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "\n",
        "# Decode\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Test Prompt:\")\n",
        "print(\"=\" * 50)\n",
        "print(test_prompt)\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Model Response:\")\n",
        "print(\"=\" * 50)\n",
        "print(response[len(test_prompt):].strip())\n",
        "print(\"\\n‚úÖ Model inference test passed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"dataset\"></a>\n",
        "## 3. Dataset Loading & Preprocessing\n",
        "\n",
        "**‚è±Ô∏è Time Estimate: 10-25 minutes** | **GPU REQUIRED**\n",
        "\n",
        "Load FiQA dataset and format for instruction fine-tuning.\n",
        "\n",
        "### FiQA Dataset Info\n",
        "- **Source**: [explodinggradients/fiqa](https://huggingface.co/datasets/explodinggradients/fiqa)\n",
        "- **Splits**: Train (5,500), Validation (500), Test (648)\n",
        "- **Format**: Question + Ground Truth Answers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load FiQA dataset\n",
        "# ‚è±Ô∏è Time: ~1 minute\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"Loading FiQA Dataset\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Load dataset with 'main' config\n",
        "fiqa_dataset = load_dataset(\"explodinggradients/fiqa\", \"main\")\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded!\")\n",
        "print(f\"\\nSplits:\")\n",
        "for split_name, split_data in fiqa_dataset.items():\n",
        "    print(f\"  {split_name}: {len(split_data):,} samples\")\n",
        "\n",
        "print(f\"\\nColumns: {fiqa_dataset['train'].column_names}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore sample data\n",
        "print(\"=\" * 50)\n",
        "print(\"Sample Data\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "sample = fiqa_dataset['train'][0]\n",
        "print(f\"\\nQuestion:\\n{sample['question']}\")\n",
        "print(f\"\\nGround Truth Answer (first of {len(sample['ground_truths'])}):\")\n",
        "print(sample['ground_truths'][0][:500] + \"...\" if len(sample['ground_truths'][0]) > 500 else sample['ground_truths'][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format data for instruction fine-tuning\n",
        "def format_for_training(example: Dict) -> Dict:\n",
        "    \"\"\"\n",
        "    Format FiQA sample for instruction fine-tuning.\n",
        "    \n",
        "    Input format:\n",
        "        - question: str\n",
        "        - ground_truths: List[str]\n",
        "    \n",
        "    Output format:\n",
        "        - text: formatted instruction string\n",
        "        - input_text: question only (for inference)\n",
        "        - target_text: answer only (for evaluation)\n",
        "    \"\"\"\n",
        "    question = example['question']\n",
        "    # Use first ground truth answer (most relevant)\n",
        "    answer = example['ground_truths'][0] if example['ground_truths'] else \"\"\n",
        "    \n",
        "    # Truncate very long answers to avoid context length issues\n",
        "    max_answer_len = 1024\n",
        "    if len(answer) > max_answer_len:\n",
        "        answer = answer[:max_answer_len] + \"...\"\n",
        "    \n",
        "    # Format as instruction-following\n",
        "    instruction = f\"\"\"You are a helpful financial assistant. Answer the following question about finance.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer: {answer}\"\"\"\n",
        "    \n",
        "    # Also store components separately for evaluation\n",
        "    input_prompt = f\"\"\"You are a helpful financial assistant. Answer the following question about finance.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    \n",
        "    return {\n",
        "        \"text\": instruction,\n",
        "        \"input_text\": input_prompt,\n",
        "        \"target_text\": answer,\n",
        "        \"question\": question,\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Formatting function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply formatting to all splits\n",
        "# ‚è±Ô∏è Time: ~1 minute\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"Formatting Dataset\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "formatted_dataset = fiqa_dataset.map(\n",
        "    format_for_training,\n",
        "    remove_columns=['ground_truths'],  # Keep only new columns\n",
        "    desc=\"Formatting samples\"\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Dataset formatted!\")\n",
        "print(f\"New columns: {formatted_dataset['train'].column_names}\")\n",
        "\n",
        "# Show formatted example\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Formatted Sample\")\n",
        "print(\"=\" * 50)\n",
        "print(formatted_dataset['train'][0]['text'][:800] + \"...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize dataset\n",
        "# ‚è±Ô∏è Time: 5-15 minutes\n",
        "\n",
        "def tokenize_function(examples: Dict) -> Dict:\n",
        "    \"\"\"Tokenize examples for training.\"\"\"\n",
        "    # Tokenize the full text (question + answer)\n",
        "    tokenized = tokenizer(\n",
        "        examples['text'],\n",
        "        truncation=True,\n",
        "        max_length=CONFIG['max_length'],\n",
        "        padding='max_length',\n",
        "        return_tensors=None,\n",
        "    )\n",
        "    \n",
        "    # For causal LM, labels are same as input_ids\n",
        "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
        "    \n",
        "    return tokenized\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"Tokenizing Dataset\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "tokenized_dataset = formatted_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['text', 'input_text', 'target_text', 'question'],\n",
        "    desc=\"Tokenizing\"\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Dataset tokenized!\")\n",
        "print(f\"Columns: {tokenized_dataset['train'].column_names}\")\n",
        "print(f\"Sample input_ids length: {len(tokenized_dataset['train'][0]['input_ids'])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset summary\n",
        "print(\"=\" * 50)\n",
        "print(\"Dataset Preparation Complete\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"\\nüìä Dataset Statistics:\")\n",
        "print(f\"   Train samples: {len(tokenized_dataset['train']):,}\")\n",
        "print(f\"   Validation samples: {len(tokenized_dataset['validation']):,}\")\n",
        "print(f\"   Test samples: {len(tokenized_dataset['test']):,}\")\n",
        "print(f\"   Max sequence length: {CONFIG['max_length']}\")\n",
        "\n",
        "# Keep the formatted dataset for evaluation later\n",
        "eval_dataset = formatted_dataset  # Has input_text and target_text\n",
        "\n",
        "print(\"\\n‚úÖ Phase 3: Dataset Loading & Preprocessing complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"baseline\"></a>\n",
        "## 4. Baseline Evaluation\n",
        "\n",
        "**‚è±Ô∏è Time Estimate: 30-60 minutes** | **GPU REQUIRED**\n",
        "\n",
        "Evaluate the base model on FiQA test set before fine-tuning.\n",
        "\n",
        "> üöß **TODO: Phase 4** - Implement baseline evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"lora-config\"></a>\n",
        "## 5. LoRA Configuration\n",
        "\n",
        "**‚è±Ô∏è Time Estimate: ~5 minutes** | **GPU REQUIRED**\n",
        "\n",
        "Configure and apply LoRA adapter to the model.\n",
        "\n",
        "> üöß **TODO: Phase 5** - Implement LoRA configuration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"training\"></a>\n",
        "## 6. LoRA Training\n",
        "\n",
        "**‚è±Ô∏è Time Estimate: 3-6 hours** | **GPU REQUIRED**\n",
        "\n",
        "Train the LoRA adapter on FiQA training data.\n",
        "\n",
        "> üöß **TODO: Phase 6** - Implement LoRA training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"evaluation\"></a>\n",
        "## 7. Fine-tuned Evaluation\n",
        "\n",
        "**‚è±Ô∏è Time Estimate: 30-60 minutes** | **GPU REQUIRED**\n",
        "\n",
        "Evaluate the fine-tuned model and compare with baseline.\n",
        "\n",
        "> üöß **TODO: Phase 7** - Implement fine-tuned evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"visualization\"></a>\n",
        "## 8. Visualization & Analysis\n",
        "\n",
        "Create visualizations comparing baseline vs fine-tuned performance.\n",
        "\n",
        "> üöß **TODO: Phase 8** - Implement visualization and analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook demonstrated LoRA fine-tuning of Nemotron Next 8B on the FiQA financial QA dataset.\n",
        "\n",
        "### Results\n",
        "\n",
        "| Metric | Baseline | Fine-tuned | Improvement |\n",
        "|--------|----------|------------|-------------|\n",
        "| Exact Match | TBD | TBD | TBD |\n",
        "| F1 Score | TBD | TBD | TBD |\n",
        "| BLEU | TBD | TBD | TBD |\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Experiment with different LoRA ranks\n",
        "- Try longer training\n",
        "- Evaluate on additional financial QA datasets\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
